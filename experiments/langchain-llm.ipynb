{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# By providing examples of extraction, get LLM to follow and extract info in JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pip installs (if required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Zain\\Documents\\Github\\University\\.venv\\Lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Zain\\Documents\\Github\\University\n",
      "c:\\Users\\Zain\\Documents\\Github\\University\\experiments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Zain\\Documents\\Github\\University\\.venv\\Lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "from src import *\n",
    "%cd experiments/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_code_from_url(url: str):\n",
    "    last_question_mark = url.rfind('?')\n",
    "    code = url[last_question_mark - 32:last_question_mark]\n",
    "    return code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(filename: str, pageID: str):\n",
    "    \"\"\"Runs full pipeline\"\"\"\n",
    "    docIngest(filename).run()\n",
    "    mdCombine(filename).run()\n",
    "    getChunks(filename).run()\n",
    "    Extractor(filename).run()\n",
    "    Squasher(filename).run()\n",
    "    Notion(filename, pageID).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116a89672d99807781b1eb665cb64ad6\n",
      "Folder found in experiments/experiment20241009124628\n",
      "Here is the extracted result in JSON format based on the provided text:\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"Random Walk on \\( \\mathbb{Z} \\)\": \"If the chain starts at 0 and returns to 0 in \\( 2n \\) steps, then the chain made \\( n \\) steps to the right each with probability \\( p \\) and \\( n \\) steps to the left, each with probability \\( q \\). Since the number of ways to choose \\( n \\) steps out of \\( 2n \\) steps is \\( \\binom{2n}{n} \\), it follows that \\( P_{00}^{2n} = \\binom{2n}{n} p^n q^n = \\frac{(2n)!}{(n!)^2} p^n q^n, \\quad n \\ge 1. \\n\\nWe use Stirling's formula to approximate \\( n! \\) for large \\( n \\): \\( n! \\sim \\sqrt{2 \\pi n} \\left( \\frac{n}{e} \\right)^n \\quad \\text{as} \\quad n \\to \\infty. \\n\\nThus, \\( P_{00}^{2n} \\sim \\frac{\\sqrt{4 \\pi n} \\left( \\frac{2n}{e} \\right)^{2n}}{2 \\pi n \\left( \\frac{n}{e} \\right)^{2n}} p^n q^n = \\frac{2^{2n}}{\\sqrt{\\pi n}} p^n q^n = \\frac{(4pq)^n}{\\sqrt{\\pi n}}, \\quad \\text{as} \\quad n \\to \\infty. \\n\\nSuppose that \\( p = q = \\frac{1}{2} \\). Then \\( P_{00}^{2n} \\sim \\frac{1}{\\sqrt{\\pi n}}, \\quad \\text{as} \\quad n \\to \\infty. \\n\\nIt follows that there exists \\( N \\in \\mathbb{N} \\) such that \\( P_{00}^{2n} > k \\) for some constant \\( k \\).\",\n",
      "    \"Recurrence/Transience\": \"A simple random walk on \\( \\mathbb{Z} \\) is \\n\\\\left\\\\{\\\\begin{array}{ll} recurrent & \\\\text{if } p = q = \\\\frac{1}{2}, \\\\\\\\ transient & \\\\text{if } p \\\\neq q. \\\\end{array}\\\\right.\",\n",
      "    \"Establishing Recurrence\": \"We will establish recurrence/transience of state 0 by establishing whether \\( \\\\sum_{n=0}^{\\\\infty} P_{00}^{n} \\\\) is finite or infinite. \\n\\nIf the random walk starts at 0, it can again visit 0 only after performing an even number of steps, hence \\( P_{00}^{2n-1} = 0, \\quad n \\\\in \\\\mathbb{N}. \\n\\nThus, \\( \\\\sum_{n=0}^{\\\\infty} P_{00}^{n} = \\\\sum_{n=0}^{\\\\infty} P_{00}^{2n}. \\n\\nIf the chain starts at 0 and returns to 0 in \\( 2n \\) steps, then the chain made \\( n \\) steps to the right each with probability \\( p \\) and \\( n \\) steps to the left, each with probability \\( q \\). Since the number of ways to choose \\( n \\) steps out of \\( 2n \\) steps is \\( \\\\binom{2n}{n} \\), it follows that \\( P_{00}^{2n} = \\\\binom{2n}{n} p^n q^n = \\\\frac{(2n)!}{(n!)^2} p^n q^n, \\quad n \\\\ge 1. \\n\\nUsing Stirling's formula, we have \\( n! \\\\sim \\\\sqrt{2 \\\\pi n} \\\\left( \\\\frac{n}{e} \\\\right)^n \\\\quad as \\\\: n \\\\to \\\\infty. \\n\\nThus, \\( P_{00}^{2n} \\\\sim \\\\frac{\\\\sqrt{4 \\\\pi n} \\\\left( \\\\frac{2n}{e} \\\\right)^{2n}}{2 \\\\pi n \\\\left( \\\\frac{n}{e} \\\\right)^{2n}} p^n q^n = \\\\frac{2^{2n}}{\\\\sqrt{\\\\pi n}} p^n q^n = \\\\frac{(4pq)^n}{\\\\sqrt{\\\\pi n}}, \\\\quad as \\\\: n \\\\to \\\\infty. \\n\\nIf \\( p = q = \\\\frac{1}{2} \\), then \\( P_{00}^{2n} \\\\sim \\\\frac{1}{\\\\sqrt{\\\\pi n}}, \\\\quad as \\\\: n \\\\to \\\\infty. \\n\\nThus, there exists \\( N \\\\in \\\\mathbb{N} \\) such that \\( P_{00}^{2n} > k \\) for some constant \\( k \\).\"\n",
      "}\n",
      "``` \n",
      "\n",
      "This JSON format encapsulates the key concepts and results related to the recurrence and transience of a random walk on the integer line as discussed in the provided text.\n",
      "{\n",
      "    \"Stochastic Process\": \"A family of random variables $${{X_{{t}}(\\\\omega)\\\\mid t\\\\in T}}$$ indexed by a set $$T$$ and defined on the same probability space $$(\\\\Omega,\\\\mathcal{{F}},\\\\mathbb{{P}})$$\",\n",
      "    \"State Space of a Stochastic Process\": \"The set of values taken by the random variables $$X_{{t}}$$, where $$t\\\\in T$$\",\n",
      "    \"Discrete Time Markov Chain\": \"Let $$(X_{{n}})_{{n\\\\geq0}}$$ be a discrete time, discrete state space stochastic process and let $$S$$ denote its state space. We say that $$(X_{{n}})_{{n\\\\geq0}}$$ is a discrete time Markov chain if it satisfies the Markov property, that is if $$$$\\\\mathbb{{P}}(X_{{n+1}}=x_{{n+1}}|X_{{n}}=x_{{n}},\\\\ldots X_{{2}}=x_{{2}},X_{{1}}=x_{{1}},X_{{0}}=x_{{0}})=\\\\mathbb{{P}}(X_{{n+1}}=x_{{n+1}}|X_{{n}}=x_{{n}}$$$$ for any $$n\\\\in\\\\mathbb{{N}}$$ and any $$x_{{0}},x_{{1}},x_{{2}},\\\\ldots x_{{n}},x_{{n+1}}\\\\in S$$\",\n",
      "    \"Time-Homogeneous Discrete Time Markov Chain\": \"Let $$(X_{{n}})_{{n\\\\geq0}}$$ be a discrete time Markov chain with the state space $$S$$. The Markov chain $$(X_{{n}})_{{n\\\\geq0}}$$ is said to be time-homogeneous if $$$$\\\\mathbb{{P}}(X_{{n+1}}=i|X_{{n}}=j)=\\\\mathbb{{P}}(X_{{1}}=i|X_{{0}}=j),\\\\:i,j\\\\in S,\\\\:n\\\\in\\\\mathbb{{N}}$$$$\",\n",
      "    \"n-step Markov Property\": \"Let $$(X_{{n}})_{{n\\\\geq0}}$$ be a discrete time stochastic process with the discrete state space $$S$$. Then $$(X_{{n}})_{{n\\\\geq0}}$$ is Markov if and only if $$$$\\\\mathbb{{P}}(X_{{n+m}}=x_{{n+m}}|X_n=x_n,\\\\ldots,X_1=x_1,X_0=x_0)=\\\\mathbb{{P}}(X_{{n+m}}=x_{{n+m}}|X_n=x_n)$$$$ for any $$n,m\\\\in\\\\mathbb{{N}}$$, any $$x_0,x_1,\\\\ldots x_n\\\\in S$$ and any $$x_{{n+m}}\\\\in S$$\",\n",
      "    \"Transition Matrix\": \"Let $$(X_{{n}})_{{n\\\\geq0}}$$ be a Markov chain with the state space $$S$$. The values $$P_{{ij}}=\\\\mathbb{{P}}(X_1=i|X_0=j),\\\\:i,j\\\\in S$$ are called one-step transition probabilities of the Markov chain $$(X_{{n}}){{n\\\\geq0}}$$. The matrix $$P\\\\:=\\\\:(P{{ij}})_{{S\\\\times S}}$$ is called the transition matrix of the Markov chain $$(X_{{n}}){{n\\\\geq0}}$$.\",\n",
      "    \"Random Walk\": \"A simple, one-dimensional random walk where $$X_0=0$$ and for each $$n\\\\in\\\\mathbb{N}$$, $$X_n=\\\\left\\\\{\\\\begin{array}{ll}X_{n-1}+1,&\\\\text{with probability } p,\\\\X_{n-1}-1,&\\\\text{with probability } 1-p.\\\\end{array}\\\\right.$$ This random walk models the movement of a particle on the integer number line.\",\n",
      "    \"Gambler's Ruin Problem\": \"The probability that the gambler starting with n goes bankrupt before reaching LN is given by v_n=A\\\\left(\\\\frac{{q}}{{p}}\\\\right)^n+B.\",\n",
      "    \"Transition Matrix Definition\": \"Let $$(X_{n})_{n\\\\geq0}$$ be a Markov chain with the state space $$S$$. The values $$P_{ij}=\\\\mathbb{P}(X_1=j|X_0=i),\\\\:i,j\\\\in S$$ are called one-step transition probabilities.\",\n",
      "    \"Substochastic / Stochastic Matrix\": \"A square matrix $$A=(A_{{ij}})_{{S\\\\times S}}$$ is called substochastic if $$0\\\\leq A_{{ij}}\\\\leq1,\\\\:i,j\\\\in S\\\\quad\\\\mathrm{{and}}\\\\quad\\\\sum_{{j\\\\in S}}A_{{ij}}\\\\leq1,\\\\:i\\\\in S$$. A square matrix $$A=(A_{{ij}})_{{S\\\\times S}}$$ is called stochastic if $$0\\\\leq A_{{ij}}\\\\leq1,\\\\:i,j\\\\in S\\\\quad\\\\mathrm{{and}}\\\\quad\\\\sum_{{j\\\\in S}}A_{{ij}}=1,\\\\:i\\\\in S$$.\",\n",
      "    \"n-Step Transition Probabilities\": \"Let $$(X_{n})_{n\\\\geq0}$$ be a Markov chain with the state space S and the one-step transition matrix $$P$$. The $$n$$-step transition probabilities are given by $$\\\\mathbb{P}(X_n=j|X_0=i)=P_{ij}^n,\\\\:i,j\\\\in S,\\\\:n\\\\in\\\\mathbb{N}$$.\",\n",
      "    \"Kolmogorov-Chapman Equations\": \"$$\\\\mathbb{{P}}(X_{{n+m}}=j|X_0=i)=\\\\sum_{{k\\\\in S}}\\\\mathbb{{P}}(X_{{n+m}}=j|X_n=k)\\\\:\\\\mathbb{{P}}(X_n=k|X_0=i)$$, $$P_{{ij}}^{{n+m}}=\\\\sum_{{k\\\\in S}}P_{{ik}}^{{n}}P_{{kj}}^{{m}}$$ for any $$i,j\\\\in S$$ and any $$n,m\\\\in\\\\mathbb{{N}}$$.\",\n",
      "    \"Initial Distribution\": \"Let $$(X_{n})_{n\\\\geq0}$$ be a Markov chain with the state space $$S$$. We say that $$\\\\lambda=(\\\\lambda_{i})_{i\\\\in S}$$ is the initial distribution of the chain if $$\\\\lambda$$ is a distribution on S and if $$\\\\mathbb{P}(X_0=i)=\\\\lambda_i,\\\\:i\\\\in S$$.\",\n",
      "    \"Forward Kolmogorov Equation\": \"Let $$(X_{{n}})_{n\\\\geq0}$$ be a Markov chain with the initial distribution $$\\\\lambda$$ and the one-step transition matrix $$P$$. The distributions of all the random variables in the chain are completely defined by $$\\\\lambda$$ and $$P$$, that is $$\\\\mathbb{{P}}_\\\\lambda(X_{{n+1}}=i)=\\\\sum_{{j\\\\in S}}\\\\mathbb{{P}}_\\\\lambda(X_{{n}}=j)\\\\:P_{{ji}}$$ for any $$n\\\\in\\\\mathbb{{N}}$$ and any $$i\\\\in S$$.\",\n",
      "    \"Markov Chain\": \"Let $$(X_{{n}})_{{n\\\\geq0}}$$ be a Markov chain with state space $$S$$ = $$\\\\{{ A, B, C\\\\}}$$ and transition matrix $$P$$.\",\n",
      "    \"Stationary Distribution\": \"$$\\\\pi=\\\\left(\\\\frac{{1}}{{4}},\\\\frac{{1}}{{4}},\\\\frac{{1}}{{2}}\\\\right)$$\",\n",
      "    \"Example 4.1.1\": \"The Markov chain is irreducible and has finite state space, thus there exists the unique stationary distribution $$\\\\pi=(\\\\pi_1,\\\\pi_2,\\\\pi_3)=\\\\left(\\\\frac{7}{23},\\\\frac{10}{23},\\\\frac{6}{23}\\\\right).$$\",\n",
      "    \"Example 4.1.2\": \"Let $$(X_{n})_{n\\\\geq0}$$ be a Markov chain with state space $$S = \\\\{ A, B, C\\\\}$$ and transition matrix $$P$$. Suppose that $$\\\\pi$$ is a solution to the equation $$\\\\pi P=\\\\pi$$. Then $$\\\\left\\\\{\\\\begin{array}{c}\\\\pi_1=\\\\pi_1\\\\\\\\0.4\\\\pi_2+0.5\\\\pi_3=\\\\pi_2\\\\\\\\0.6\\\\pi_2+0.5\\\\pi_3=\\\\pi_3\\\\end{array}\\\\right.\\\\quad\\\\implies\\\\quad\\\\left\\\\{\\\\begin{array}{c}\\\\pi_1=a\\\\\\\\\\\\pi_2=b\\\\\\\\\\\\pi_3=1.2b,\\\\end{array}\\\\right.\\\\quad a,b\\\\in\\\\mathbb{R}$$. We want a solution which is a distribution on $$S$$, that is which satisfies $$\\\\pi_1,\\\\pi_2,\\\\pi_3\\\\in[0,1]$$ and $$\\\\sum_{i=1}^{3}\\\\pi_{i}=1$$. We have that $$1=\\\\sum_{i=1}^3\\\\pi_i=a+b+1.2b=a+2.2b$$. Hence, any solution of the form $$\\\\pi=(\\\\pi_1,\\\\pi_2,\\\\pi_3)=\\\\left(a,\\\\frac{5(1-a)}{11},\\\\frac{6(1-a)}{11}\\\\right),\\\\:a\\\\in[0,1]$$ is a distribution on $$S$$ and satisfies $$\\\\pi P=\\\\pi$$, which implies that it is a stationary distribution for $$(X_{n})_{n\\\\geq0}$$.\",\n",
      "    \"Example 4.1.3\": \"Let $$p\\\\in(0,1)$$ and let $$q=1-p$$. Let $$(X_{n})_{n\\\\geq0}$$ be a simple random walk on $$Z$$ with transition matrix $$P$$ such that $$P_{ij}=\\\\left\\\\{\\\\begin{array}{cc}p,&j=i+1,\\\\\\\\q,&j=i-1,\\\\\\\\0,&otherwise,\\\\end{array}\\\\right.\\\\quad i,j\\\\in\\\\mathbb{Z}$$. Suppose that $$\\\\pi P=\\\\pi$$. Then $$p\\\\pi_{i-1}+q\\\\pi_{i+1}=\\\\pi_{i},\\\\:i\\\\in\\\\mathbb{Z}$$. Suppose that $$p\\\\neq q$$. The solution is of the form $$\\\\pi_i=A+B\\\\left(\\\\frac{p}{q}\\\\right)^i,\\\\quad i\\\\in\\\\mathbb{Z}$$ for some constants $$A\\\\in\\\\mathbb{R}$$ and $$B\\\\in\\\\mathbb{R}$$. For a choice of $$A\\\\geq0$$ and $$B\\\\geq0$$, $$\\\\pi=\\\\{\\\\pi_{i},i\\\\in\\\\mathbb{Z}\\\\}$$ is an invariant measure. However, for any choice of $$A\\\\geq0$$ and $$B\\\\geq0$$ (except $$A=B=0$$) $$\\\\sum_{i=-\\\\infty}^{\\\\infty}\\\\pi_{i}$$ is not finite, and when $$A=B=0$$ then $$\\\\pi_{i}=0$$, $$i\\\\in\\\\mathbb{Z}$$. It follows that such $$\\\\pi$$ cannot be a distribution on $$S$$. We deduce that there is no stationary distribution for $$(X_{n})_{n\\\\geq0}$$.\",\n",
      "    \"No Stationary Distribution\": \"It follows that such $$\\\\pi$$ cannot be a distribution on $$S$$. We deduce that there is no stationary distribution for $$(X_{n})_{n\\\\geq0}$$.\",\n",
      "    \"Markov Chain Transition Matrix\": \"Let $$(X_{n})_{n\\\\geq0}$$ be a Markov chain with state space $$S = \\\\{ A, B, C \\\\}$$ and transition matrix $$P$$ such that $$\\\\left\\\\{\\\\begin{array}{c}\\\\pi_1=\\\\pi_1\\\\\\\\0.4\\\\pi_2+0.5\\\\pi_3=\\\\pi_2\\\\\\\\0.6\\\\pi_2+0.5\\\\pi_3=\\\\pi_3\\\\end{array}\\\\right.$$.\",\n",
      "    \"General Solution for Transition Matrix\": \"Suppose that $$\\\\pi P=\\\\pi$$. Then the system of equations is given by $$\\\\left\\\\{\\\\begin{array}{rl}0.4\\\\pi_2+0.5\\\\pi_3=\\\\pi_1\\\\\\\\\\\\pi_1+0.5\\\\pi_3=\\\\pi_2\\\\\\\\0.6\\\\pi_2=\\\\pi_3\\\\end{array}\\\\right.$$. Hence, we have infinitely many solutions of the equation $$\\\\pi P=\\\\pi$$.\",\n",
      "    \"Invariant Measure\": \"Let $$(X_{{n}})_{{n\\\\geq0}}$$ be a Markov chain with state space $$S$$ and transition matrix $$P$$. We say that $$\\\\mu={{\\\\mu_{{i}},i\\\\in S}}$$ is an invariant measure for transition matrix $$P$$ if $$\\\\mu_{{i}}\\\\geq0$$, $$i\\\\in S$$ and $$\\\\mu_{{j}}=\\\\sum_{{i\\\\in S}}\\\\mu_{{i}}P_{{ij}},\\\\quad j\\\\in S$$ or, equivalently, if $$\\\\mu=\\\\mu P$$.\",\n",
      "    \"Condition for Stationary Distribution\": \"The condition for a stationary distribution is given by $$1=\\\\sum_{{i=1}}^3\\\\pi_i=a+b+1.2b=a+2.2b$$.\",\n",
      "    \"Unique Stationary Distribution\": \"If $$p=q={{\\\\frac{{1}}{{2}}}$$, the solution is of the form $$\\\\pi_i=A+Bi,\\\\quad i\\\\in\\\\mathbb{{Z}}$$. Since $$p_{{i}}\\\\geq0$$, it follows that $$B=0$$, leading to a unique stationary distribution.\",\n",
      "    \"Distribution Condition\": \"We want a solution that is a distribution on $$S$$, that is which satisfies $$\\\\pi_1,\\\\pi_2,\\\\pi_3\\\\in[0,1]$$ and $$\\\\sum_{{i=1}}^{{3}}\\\\pi_{{i}}=1$$.\",\n",
      "    \"Example of Stationary Distribution\": \"Any solution of the form $$\\\\pi=(\\\\pi_1,\\\\pi_2,\\\\pi_3)=\\\\left(a,\\\\frac{{5(1-a)}}{{11}},\\\\frac{{6(1-a)}}{{11}}\\\\right),\\\\:a\\\\in[0,1]$$ is a distribution on $$S$$ and satisfies $$\\\\pi P=\\\\pi$$.\",\n",
      "    \"Infinitely Many Stationary Distributions\": \"For instance, $$(1,0,0),\\\\quad\\\\left(0,\\\\frac{{5}}{{11}},\\\\frac{{6}}{{11}}\\\\right),\\\\quad\\\\left(\\\\frac{{1}}{{2}},\\\\frac{{5}}{{22}},\\\\frac{{6}}{{22}}\\\\right)$$ are examples of a stationary distribution for $$(X_{{n}})_{{n\\\\geq0}}$$. We deduce that there are infinitely many stationary distributions for $$(X_{{n}})_{{n\\\\geq0}}$$.\",\n",
      "    \"Simple Random Walk\": \"Let $$(X_{{n}})_{{n\\\\geq0}}$$ be a simple random walk on $$Z$$ with transition matrix $$P$$ such that $$P_{{ij}}=\\\\left\\\\{{\\\\begin{{array}}{{cc}}p,&j=i+1,\\\\\\\\q,&j=i-1,\\\\\\\\0,&otherwise,\\\\end{{array}}\\\\right.\\\\quad i,j\\\\in\\\\mathbb{{Z}}.$$$$\",\n",
      "    \"Solution for p \\u2260 q\": \"The solution is of the form $$$$\\\\pi_i=A+B\\\\left(\\\\frac{p}{q}\\\\right)^i,\\\\quad i\\\\in\\\\mathbb{Z}$$$ for some constants $$A\\\\in\\\\mathbb{R}$$ and $$B\\\\in\\\\mathbb{R}$$.\",\n",
      "    \"Invariant Measure Conditions\": \"For a choice of $$A \\\\geq 0$$ and $$B \\\\geq 0$$, $$\\\\pi=\\\\{\\\\pi_{i},i\\\\in\\\\mathbb{Z}\\\\}$$ is an invariant measure. However, for any choice of $$A \\\\geq 0$$ and $$B \\\\geq 0$$ (except $$A=B=0$$), $$\\\\sum_{i=-\\\\infty}^{\\\\infty}\\\\pi_{i}$$ is not finite, and when $$A=B=0$$ then $$\\\\pi_{i}=0$$, $$i\\\\in\\\\mathbb{Z}$$.\",\n",
      "    \"Solution for p = q\": \"Suppose that $$p=q=\\\\frac{1}{2}$$. The solution is of the form $$$$\\\\pi_i=A+Bi,\\\\quad i\\\\in\\\\mathbb{Z}$$$ for some constants $$A\\\\in\\\\mathbb{R}$$ and $$B\\\\in\\\\mathbb{R}$$.\",\n",
      "    \"B = 0\": \"Since $$\\\\pi_{i} \\\\geq 0$$, it follows that $$B=0$$.\",\n",
      "    \"Stationary Distribution Example\": \"Hence, any solution of the form $$$$\\\\pi=(\\\\pi_1,\\\\pi_2,\\\\pi_3)=\\\\left(a,\\\\frac{5(1-a)}{11},\\\\frac{6(1-a)}{11}\\\\right),\\\\:a\\\\in[0,1]$$$ is a distribution on $$S$$ and satisfies $$\\\\pi P=\\\\pi$$, which implies that it is a stationary distribution for $$(X_{n})_{n\\\\geq0}$$.\",\n",
      "    \"Examples of Stationary Distribution\": \"For instance, $$(1,0,0),\\\\quad\\\\left(0,\\\\frac{5}{11},\\\\frac{6}{11}\\\\right),\\\\quad\\\\left(\\\\frac{1}{2},\\\\frac{5}{22},\\\\frac{6}{22}\\\\right)$$ are examples of a stationary distribution for $$(X_{n})_{n\\\\geq0}$$. We deduce that there are infinitely many stationary distributions for $$(X_{n})_{n\\\\geq0}$$.\",\n",
      "    \"Stationary Distribution Absence\": \"For any choice of $$A\\\\geq0$$ and $$B\\\\geq0$$ (except $$A=B=0$$), $$\\\\sum_{{i=-\\\\infty}}^{{\\\\infty}}\\\\pi_{{i}}$$ is not finite, and when $$A=B=0$$ then $$\\\\pi_{{i}}=0$$ for all $$i\\\\in\\\\mathbb{Z}$$. Hence, such $$\\\\pi$$ cannot be a distribution on $$S$$. We deduce that there is no stationary distribution for $$(X_{{n}})_{{n\\\\geq0}}$$.\",\n",
      "    \"Form of Solution for p = q\": \"Suppose that $$p=q={{\\\\frac{{1}}{{2}}}}$$. The solution is of the form $$$$\\\\pi_i=A+Bi,\\\\quad i\\\\in\\\\mathbb{Z}$$$$ for some constants $$A\\\\in\\\\mathbb{R}$$ and $$B\\\\in\\\\mathbb{R}$$. Since $$p_{{i}}\\\\geq0$$, it follows that $$B=0$$. Hence, for any choice of $$A$$, we have that $$\\\\pi P=\\\\pi$$.\",\n",
      "    \"Expected Number of Visits\": \"Define $$\\\\gamma_{{i}}^{{k}}$$, $$i,k\\\\in S$$, to be the expected number of visits to state $$i$$ before returning to state $$k$$: $$$$\\\\gamma_i^k=\\\\mathbb{{E}}\\\\left(\\\\sum_{{n=0}}^{{T_k-1}}\\\\mathbb{{I}}_{{\\\\{{X_n=i\\\\}}}}\\\\mid X_0=k\\\\right),\\\\quad i,k\\\\in S$$$$.\",\n",
      "    \"Proposition 4.1.2\": \"Let $$(X_{{n}})_{{n\\\\geq0}}$$ be an irreducible and recurrent Markov chain with state space $$S$$ and transition matrix $$P$$. Then, for every $$k\\\\in S$$, $$\\\\gamma^k=\\\\gamma^kP,\\\\quad k\\\\in S$$. Furthermore, $$0<\\\\gamma_{{i}}^{{k}}<\\\\infty,\\\\quad i,k\\\\in S$$.\",\n",
      "    \"Proposition 4.1.3\": \"Let $$(X_{{n}})_{{n\\\\geq0}}$$ be an irreducible Markov chain with state space S and transition matrix $$P$$. Let $$\\\\mu$$ be an invariant measure for $$P$$ such that $$\\\\mu_{{k}}=1$$ for some $$k\\\\in S$$. Then $$\\\\mu\\\\geq\\\\gamma^k$$. If, in addition, $$(X_{{n}})_{{n\\\\geq0}}$$ is recurrent, then $$\\\\mu=\\\\gamma^{{k}}$$.\",\n",
      "    \"Irreducible Markov Chain\": \"A Markov chain is called irreducible if the entire state space is one communicating class, that is if all states communicate.\",\n",
      "    \"Recurrent Markov Chain\": \"Let $$(X_{{n}})_{{n\\\\geq0}}$$ be an irreducible and recurrent Markov chain with state space $$S$$ and transition matrix $$P$$. It can be deduced from the above two propositions that (a) for each $$k\\\\in S$$, $$\\\\gamma^{{k}}$$ is an invariant measure for $$P$$ (Proposition 4.1.2), hence there exists a non-zero invariant measure for $$P$$.\",\n",
      "    \"Proportional Invariant Measures\": \"All non-zero invariant measures for $$P$$ are proportional (Proposition 4.1.3). More precisely, if $$\\\\mu$$ is an invariant measure for $$P$$ such that $$\\\\mu_{{k}}$$, then $$\\\\mu$$ is proportional to $$\\\\gamma^{{k}}$$.\",\n",
      "    \"First Return Time\": \"Recall from Definition 3.5.1 that $$T_{{k}}$$ is the first return time of the chain to state $$k$$: $$T_k=\\\\min\\\\{{n\\\\in\\\\mathbb{{N}}:X_n=k\\\\}},\\\\quad k\\\\in S$$.\",\n",
      "    \"Expected Visits\": \"Define $$\\\\gamma_{{i}}^{{k}}$$, $$i,k\\\\in S$$, to be the expected number of visits to state $$i$$ before returning to state $$k$$: $$\\\\gamma_i^k=\\\\mathbb{{E}}\\\\left(\\\\sum_{{n=0}}^{{T_k-1}}\\\\mathbb{{I}}_{{\\\\{{X_n=i\\\\}}}}\\\\mid X_0=k\\\\right),\\\\quad i,k\\\\in S$$.\",\n",
      "    \"Sum of Expected Visits\": \"Observe that $$\\\\sum_{{i\\\\in S}}\\\\gamma_{{i}}^{{k}}=1+\\\\mathbb{{E}}(T_{{k}}|X_{{0}}=k)=m_{{k}}$$.\",\n",
      "    \"Proposition 3.6.2\": \"Every recurrent communicating class is closed.\",\n",
      "    \"Proposition 3.6.3\": \"Let $$(X_{n})_{n\\\\geq0}$$ be a Markov chain with state space $$S$$. If $$i$$ is recurrent and $$i\\\\to j$$, then (a) $$j\\\\rightarrow i$$ and $$j$$ is recurrent; (b) $$\\\\mathbb{P}(H_{i}<\\\\infty|X_{0}=j)=1$$; (c) $$\\\\mathbb{P}(H_{j}<\\\\infty|X_{0}=i)=1$$.\",\n",
      "    \"Proposition 3.6.4\": \"Let $$(X_{{n}}) {{n\\\\geq0}}$$ be a Markov chain with finite state space $$S$$. Then $$(X {{n}})_{{n\\\\geq0}}$$ has at least one recurrent state.\",\n",
      "    \"Proposition 3.6.6\": \"Every finite closed communicating class is recurrent.\",\n",
      "    \"Proposition 4.1.1\": \"Let $$(X_{{n}})_{{n\\\\geq0}}$$ be a Markov chain with state space S and transition matrix $$P$$. Suppose that there exists a stationary distribution $$\\\\pi$$ for the chain and suppose that the initial distribution of $$(X_{{n}}) {{n\\\\geq0}}$$ is $$\\\\pi$$. Then, for every $$m\\\\in\\\\mathbb{{N}}$$, $$(X {{n+m}})_{{n\\\\geq0}}$$ is a Markov chain with state space $$S$$, transition matrix $$P$$ and initial distribution $$\\\\pi$$.\",\n",
      "    \"Definition 4.1.3\": \"A state $$i\\\\in S$$ is said to be positive recurrent if it is recurrent and $$\\\\mathbb{{E}}(T_i|X_0=i)=m_i<\\\\infty$$. A state $$i\\\\in S$$ is said to be null recurrent if it is recurrent and $$\\\\mathbb{{E}}(T_i|X_0=i)=m_i=\\\\infty$$.\",\n",
      "    \"Conclusions\": \"If a Markov chain is irreducible and recurrent, it has a unique stationary distribution if it is positive recurrent. If it is null recurrent, there is no stationary distribution.\",\n",
      "    \"Proposition 4.1.6\": \"Let $$(X_{{n}})_{n \\\\geq 0}$$ be an irreducible Markov chain with transition matrix $$P$$. Then there exists a stationary distribution for $$P$$ if and only if the chain is positive recurrent.\",\n",
      "    \"Proposition 4.1.4\": \"If $$(X_{n})_{n \\\\geq 0}$$ is positive recurrent, then all states in $$S$$ are positive recurrent.\",\n",
      "    \"Proposition 4.1.5\": \"If $$m_k=\\\\sum_{{i\\\\in S}}\\\\gamma_i^k<\\\\infty$$, then state $$k\\\\in S$$ is positive recurrent.\",\n",
      "    \"Expected Return Time\": \"The expected return time to each state is given by $$m_k=\\\\mathbb{E}(T_k|X_0=k)=\\\\frac{1}{\\\\pi_k},\\\\quad k\\\\in S.$$\",\n",
      "    \"Example 4.2.1\": \"Let $$(X_{n})_{n \\\\geq 0}$$ be a Markov chain with state space $$S={{1,2,3}}$$ and transition matrix $$P$$ given by $$P=\\\\begin{bmatrix}0&1&0\\\\0.4&0&0.6\\\\0.5&0.5&0\\\\end{bmatrix}.$$\",\n",
      "    \"Limiting Distribution\": \"If the Markov chain starts in a stationary distribution $$\\\\pi$$, then $$\\\\mathbb{P}_{\\\\pi}(X_{n}=j)=\\\\pi_{j}$$ for every $$j \\\\in S$$ and every $$n \\\\in \\\\mathbb{N}$$.\",\n",
      "    \"Proposition 4.2.1\": \"Let $$(X_{n})_{n \\\\geq 0}$$ be an irreducible aperiodic Markov chain with state space $$S$$ and transition matrix $$P$$. Then for any initial distribution $$\\\\lambda$$, $$\\\\lim_{n \\\\to \\\\infty}\\\\mathbb{P}_{\\\\lambda}(X_{n}=j)=\\\\frac{1}{m_{j}},\\\\:j \\\\in S.$$\",\n",
      "    \"Proposition 4.2.2\": \"Let $$(X_{n})_{n \\\\geq 0}$$ be a Markov chain with state space $$S$$ and transition matrix $$P$$. If $$j \\\\in S$$ is transient or null recurrent, then $$\\\\lim_{n \\\\to \\\\infty}P_{ij}^{n}=0,\\\\quad\\\\text{for any } i \\\\in S.$$\",\n",
      "    \"Example 4.2.2\": \"The Markov chain is periodic with period 3, thus $$\\\\operatorname{lim}_{n \\\\to \\\\infty}\\\\mathbb{P}_{\\\\lambda}(X_{n}=j)$$ does not exist.\",\n",
      "    \"Example 4.2.3\": \"Let $$(X_{n})_{n \\\\geq 0}$$ be a Markov chain with state space $$S={{1,2,3,4}}$$ and transition matrix $$P$$ given by $$P=\\\\begin{bmatrix}0&0&1&0\\\\0&0&0.4&0.6\\\\0.5&0.5&0&0\\\\1&0&0&0\\\\end{bmatrix}.$$\",\n",
      "    \"Example 4.2.4\": \"Let $$(X_{n})_{n \\\\geq 0}$$ be a simple random walk on $$Z$$. Since it is periodic with period 2, there does not exist $$\\\\operatorname{lim}_{n \\\\to \\\\infty}\\\\mathbb{P}_{\\\\lambda}(X_{n}=j)$$ for arbitrary initial distribution $$\\\\lambda$$.\",\n",
      "    \"Example 4.2.5\": \"The Markov chain has two closed classes: $$\\\\{A\\\\}$$ and $$\\\\{B,C\\\\}$$. If the chain starts in class $$A$$, it stays in it forever; if it starts in class $$B,C$$, it stays in it forever.\",\n",
      "    \"Ergodic Theorem\": \"Let $$(X_{{n}})_{{n\\\\geq1}}$$ be an irreducible Markov chain with state space S and transition matrix $$P$$. Then, for any arbitrary initial distribution $$\\\\lambda$$, $$\\\\mathbb{{P}}_\\\\lambda\\\\left(\\\\frac{{V_i(n)}}{{n}}\\\\to\\\\frac{{1}}{{m_i}}\\\\quad\\\\text{{as}}\\\\quad n\\\\to\\\\infty\\\\right)=1,\\\\quad i\\\\in S.$$\",\n",
      "    \"Proposition 4.2.3\": \"If $$j \\\\in S$$ is transient, then $$\\\\lim_{n \\\\to \\\\infty}P_{ij}^{n}=0$$ for any $$i \\\\in S$$.\",\n",
      "    \"Time-homogeneous Discrete Time Markov Chain\": \"Let $$(X_{{n}})_{{n\\\\geq0}}$$ be a discrete time Markov chain with the state space $$S$$. The Markov chain $$(X_{{n}})_{{n\\\\geq0}}$$ is said to be time-homogeneous if $$$$\\\\mathbb{{P}}(X_{{n+1}}=i|X_n=j)=\\\\mathbb{{P}}(X_1=i|X_0=j),\\\\:i,j\\\\in S,\\\\:n\\\\in\\\\mathbb{{N}}.$$$$\",\n",
      "    \"n-step Transition Probabilities\": \"Let $$(X_{{n}})_{{n\\\\geq0}}$$ be a Markov chain with the state space $$S$$ and the one-step transition matrix $$P$$. The $$n$$-step transition probabilities of the Markov chain $$(X_{{n}})_{{n\\\\geq0}}$$ are given by $$\\\\mathbb{{P}}(X_n=j|X_0=i)=P_{{ij}}^n,\\\\:i,j\\\\in S,\\\\:n\\\\in\\\\mathbb{{N}}$$, where $$P^{{m}}$$ is the $$m$$-th power of the matrix $$P$$ and $$P_{{ij}}^{{m}}$$ denotes the $$(i,j)$$ component of the matrix $$P^{{m}}$$.\",\n",
      "    \"Time Reversal of a Markov Chain\": \"An interesting property of Markov chains is that if a Markov chain starts with stationary distribution and is reversed from a fixed time, the resulting process is again a Markov chain with the same stationary distribution.\",\n",
      "    \"Proposition 5.1.1\": \"Let $$(X_{{n}})_{{n\\\\geq0}}$$ be an irreducible Markov chain with state space $$S$$ , transition matrix $$P$$ , stationary distribution $$\\\\pi$$ and initial distribution $$\\\\lambda$$. For arbitrary $$N\\\\in\\\\mathbb{{N}}$$ set $$\\\\hat{{X}} {{n}}=$$ $$X {{N-n}}$$, $$0\\\\leq n\\\\leq N$$.\",\n",
      "    \"Random Walk on Graph\": \"Let $$G=(V,E)$$ be a graph with the weight function $$w:E\\\\mapsto(0,\\\\infty)$$. Let $$(X_{{n}}) {{n\\\\geq0}}$$ be a Markov chain with state space $$V$$ and the transition matrix $$P=(P_{{x,y}})_{{V\\\\times V}}$$ given by $$P_{{x,y}}=\\\\frac{{w(x,y)}}{{W(x)}},\\\\:x,y\\\\in V$$. Then $$(X_{{n}}) {{n\\\\geq0}}$$ is called a random walk on graph $$G$$.\",\n",
      "    \"Transition Matrix P\": \"$$$$P=\\\\begin{{bmatrix}}0&\\\\frac{1}{3}&\\\\frac{2}{3}\\\\\\\\\\\\frac{1}{3}&0&\\\\frac{2}{3}\\\\\\\\\\\\frac{1}{3}&\\\\frac{1}{3}&\\\\frac{1}{3}\\\\end{{bmatrix}}.$$$$\",\n",
      "    \"Irreducible and Reversible\": \"By Exercise 5.2.2 $$(X_{{n}}) {{n\\\\geq0}}$$ is irreducible and reversible.\",\n",
      "    \"Detailed Balance Equations\": \"$$$$\\\\begin{{aligned}}\\\\pi_1P_{{1,2}}&=\\\\pi_2P_{{2,1}}=\\\\frac{{1}}{{12}},\\\\quad\\\\pi_2P_{{2,3}}=\\\\pi_3P_{{3,2}}=\\\\frac{{1}}{{6}}\\\\\\\\\\\\pi_1P_{{1,3}}&=\\\\pi_3P_{{3,1}}=\\\\frac{{1}}{{6}},\\\\quad\\\\pi_3P_{{3,3}}=\\\\frac{{1}}{{2}}\\\\times\\\\frac{{1}}{{3}}=\\\\frac{{1}}{{6}}\\\\end{{aligned}}$$$$\",\n",
      "    \"Weight Function for Graph\": \"The weight function is determined uniquely up to a constant.\",\n",
      "    \"Random Walk on a Graph\": \"$$(X {{n}})_{{n\\\\geq0}}$$ is a random walk on a graph.\",\n",
      "    \"Time-Reversal of a Markov Chain\": \"Let $$(X_{{n}})_{{n\\\\geq0}}$$ be an irreducible Markov chain with state space $$S$$ , transition matrix $$P$$ and initial distribution $$\\\\lambda$$.\",\n",
      "    \"Reversible Markov Chain\": \"We say that the chain $$(X_{{n}}) {{n\\\\geq0}}$$ is reversible if for an arbitrary $$N\\\\in\\\\mathbb{{N}}$$, $$(X {{N-n}})_{{0\\\\leq n\\\\leq N}}$$ is also a Markov chain with transition matrix $$P$$ and initial distribution $$\\\\lambda$$.\",\n",
      "    \"Definition 5.2.1 - Detailed Balance\": \"We say that $$P$$ and $$\\\\lambda$$ are in detailed balance if $$$$\\\\lambda_{{j}}P_{{ji}}=\\\\lambda_{{i}}P_{{ij}},\\\\:i,j\\\\in S.$$$$\",\n",
      "    \"Proposition 5.2.1\": \"If $$P$$ and $$\\\\lambda$$ are in detailed balance, then $$\\\\lambda$$ is the stationary distribution for $$P$$ and $$(X {{n}})_{{n\\\\geq0}}$$ is reversible.\",\n",
      "    \"Branching Process Definition\": \"Let $$X_{{0}}\\\\in\\\\mathbb{{N}}$$. Let $$Y$$ be a random variable such that $$\\\\mathbb{{P}}(Y\\\\in\\\\mathbb{{N}}\\\\cup{{0}})=1$$.\",\n",
      "    \"Extinction Probability\": \"The extinction probability $$\\\\alpha=\\\\mathbb{{P}}(H_0<\\\\infty|X_0=1)$$ is the smallest non-negative solution of the equation $$$$s=G(s).$$$$\",\n",
      "    \"Extinction Probability Definition\": \"$$$$\\\\alpha=\\\\mathbb{{P}}(H_0<\\\\infty|X_0=1).$$$$\",\n",
      "    \"Example 6.2.1\": \"If $$\\\\mathbb{{P}}(Y=0)=0$$, then $$\\\\alpha=0$$.\",\n",
      "    \"Example 6.2.2\": \"If $$\\\\mathbb{{P}}(Y=0)=1$$, then $$\\\\alpha=1$$.\",\n",
      "    \"Example 6.2.3\": \"If $$Y$$ follows a geometric distribution, extinction probability depends on the value of $$p$$.\",\n",
      "    \"Example 6.2.4\": \"Extinction probability for distribution $$\\\\mathbb{{P}}(Y=0)=\\\\mathbb{{P}}(Y=4)=\\\\frac{1}{2}$$.\",\n",
      "    \"Conclusion on Extinction Probability\": \"If $$\\\\mathbb{{E}}(Y)=\\\\mu<1$$, then the branching process gets extinct with probability 1.\",\n",
      "    \"Probability Generating Function\": \"Let $$G(s)$$ be the probability generating function of the offspring distribution $$Y$$, defined as $$$$G(s)=\\\\mathbb{{E}}(s^Y),\\\\:|s|\\\\leq1.$$$$$\",\n",
      "    \"Branching Process\": \"A branching process $$X_{{0}}\\\\in\\\\mathbb{{N}}$$ is defined by $$$$X_{{n+1}}=\\\\sum\\\\limits_{{k=1}}^{{X_n}}Y_k,\\\\:n\\\\in\\\\mathbb{{N}}\\\\cup{{0}}$$$$ where $${{Y_{{i}},i\\\\in\\\\mathbb{{N}}}}$$ are independent copies of $$Y$$.\",\n",
      "    \"Example 6.2.5\": \"Let $$(X_{{n}})_{n\\\\geq0}$$ be a branching process with $$X_0=1$$ and the offspring distribution $$Y$$ such that $$Y\\\\sim$$ Geometric $$(p)$$ for some $$p\\\\in(0,1)$$.\",\n",
      "    \"Proposition 6.2.2\": \"If $$\\\\mu=\\\\mathbb{{E}}(Y)$$, then: (a) $$p<\\\\frac{1}{2} \\\\Longrightarrow \\\\mu>1 \\\\Longrightarrow \\\\alpha<1$$; (b) $$p\\\\geq\\\\frac{1}{2} \\\\Longrightarrow \\\\mu\\\\leq1 \\\\Longrightarrow \\\\alpha=1$$.\",\n",
      "    \"Exercise 6.2.2\": \"Let $$(X_{{n}})_{n\\\\geq0}$$ be a branching process with $$X_0=1$$ and the offspring distribution $$Y$$ such that $$\\\\mathbb{{P}}(Y=1)=1$$. Determine the probability of the extinction of the process.\",\n",
      "    \"Solution Exercise 6.2.2\": \"Since $$\\\\mathbb{{P}}(Y=0|X_0=1)=0$$, it follows that the extinction probability is $$\\\\alpha=0$$.\",\n",
      "    \"Exercise 6.2.3\": \"Let $$(X_{{n}})_{n\\\\geq0}$$ be a branching process with $$X_0=1$$ and the offspring distribution $$Y$$ such that $$\\\\mathbb{{P}}(Y=k)=1$$ for some fixed $$k\\\\in\\\\mathbb{{N}}, k\\\\geq2$$.\",\n",
      "    \"Solution Exercise 6.2.3\": \"Since $$\\\\mathbb{{P}}(Y=0|X_0=1)=0$$, the extinction probability is $$\\\\alpha=0$$.\",\n",
      "    \"Exercise 6.2.4\": \"Let $$(X_{{n}})_{n\\\\geq0}$$ be a branching process with $$X_0=1$$ and the offspring distribution $$Y$$ such that $$\\\\mathbb{{P}}(Y=0)=1-p$$ and $$\\\\mathbb{{P}}(Y=1)=p$$.\",\n",
      "    \"Solution Exercise 6.2.4(a)\": \"$$G(s)=\\\\mathbb{{E}}(s^Y)=(1-p)+ps,\\\\quad s\\\\in\\\\mathbb{{R}}.$$$$\",\n",
      "    \"Solution Exercise 6.2.4(b)\": \"The extinction probability can be computed using the generated function.\",\n",
      "    \"Exercise 6.2.5\": \"Let $$(X_{{n}})_{n\\\\geq0}$$ be a branching process with $$X_0=1$$ and offspring distribution with probability generating function $$G(s)=a+bs+cs^2$$.\",\n",
      "    \"Solution Exercise 6.2.5\": \"Based on the values of $$a, b, c$$, the extinction probability can be determined.\",\n",
      "    \"Exercise 6.2.6\": \"Let $$(X_{{n}})_{n\\\\geq0}$$ be a branching process with $$X_0=1$$ and the offspring distribution $$Y\\\\sim$$ Geometric $$\\\\left(\\\\frac{1}{2}\\\\right)$$.\",\n",
      "    \"Solution Exercise 6.2.6\": \"(a) Show that $$F_n(s)=\\\\frac{{n-(n-1)s}}{{(n+1)-ns}},\\\\quad n\\\\in\\\\mathbb{{N}}.$$\",\n",
      "    \"Exercise 6.2.8\": \"Let $$(X_{{n}})_{n\\\\geq0}$$ be a branching process with $$X_0=1$$ and $$\\\\mathbb{{P}}(Y=1)\\\\neq1$$.\",\n",
      "    \"Solution Exercise 6.2.8\": \"Show that every non-zero state of $$(X_{{n}})_{n\\\\geq0}$$ is transient.\",\n",
      "    \"Distribution\": \"Let $$S$$ be a countable set. We say that $$\\\\lambda=(\\\\lambda_{{i}})_{{i\\\\in S}}$$ is a distribution on $$S$$ if $$\\\\lambda_i\\\\in[0,1],\\\\:i\\\\in S,\\\\quad\\\\mathrm{{and}}\\\\quad\\\\sum_{{i\\\\in S}}\\\\lambda_i=1$$.\",\n",
      "    \"Initial Distribution of a Markov Chain\": \"Let $$(X_{{n}}){{n\\\\geq0}}$$ be a Markov chain with the state space $$S$$. We say that $$\\\\lambda=(\\\\lambda{{i}})_{{i\\\\in S}}$$ is the initial distribution of the chain $$(X_{{n}}){{n\\\\geq0}}$$ if $$\\\\lambda$$ is a distribution on $$S$$ and if $$\\\\mathbb{{P}}(X_0=i)=\\\\lambda_i,\\\\:i\\\\in S$$.\",\n",
      "    \"Theorem on Markov Chain\": \"For a given set $$S$$, let $$P$$ be a stochastic matrix on $$S\\\\times S$$ and let $$\\\\lambda$$ be a distribution on $$S$$. $$(X_{{n}})_{n\\\\geq0}$$ is a Markov chain with the initial distribution $$\\\\lambda$$ and the one-step transition matrix $$P$$ if and only if $$\\\\mathbb{{P}}_\\\\lambda(X_{n}=i_{n},\\\\ldots,X_{1}=i_{1},X_{0}=i_{0})=\\\\lambda_{i_{0}}\\\\:P_{i_{0}i_{1}}\\\\:\\\\ldots P_{i_{n-2}i_{n-1}}\\\\:P_{i_{n-1}i_{n}}$$ for any $$n\\\\in\\\\mathbb{{N}}$$ and any $$i_{0},i_{1},\\\\ldots i_{n}\\\\in S$$.\",\n",
      "    \"Communicating States\": \"Let $$(X_{{n}})_{n\\\\geq0}$$ be a Markov chain with the state space $$S$$ and the transition matrix $$P$$. Let $$i,j\\\\in S$$. We say that state $$j$$ is accessible from state $$i$$ and write $$i\\\\to j$$ if there exists $$n\\\\in\\\\mathbb{{N}}\\\\cup{0}$$ such that $$P_{ij}^{{n}}>0$$. If $$i\\\\rightarrow j$$ and $$j\\\\rightarrow i$$ then we say that the states $$i$$ and $$j$$ communicate and write $$i\\\\leftrightarrow j$$.\",\n",
      "    \"Communicating Class\": \"Let $$(X_{{n}})_{n\\\\geq0}$$ be a Markov chain with the state space $$S$$ and the transition matrix $$P$$. A communicating class $$C\\\\subseteq S$$ is a maximal collection of communicating states such that for any $$i,j\\\\in C$$, $$i\\\\leftrightarrow j$$, and for any $$k\\\\notin C$$ there does not exist a state in $$C$$ which communicates with $$k$$.\",\n",
      "    \"Closed Class / Absorbing State\": \"A communicating class is called closed if no states outside the class are accessible from the states within the class. A state is called absorbing if it forms a closed class by itself.\",\n",
      "    \"Period of a State\": \"Let $$(X_{{n}})_{n\\\\geq0}$$ be a Markov chain with the state space $$S$$ and the transition matrix $$P$$. For any state $$i\\\\in S$$, the period $$d_{i}$$ of state $$i$$ is given by $$d_i=\\\\gcd{{n\\\\in\\\\mathbb{{N}}:P_{ii}^n>0}}$$. If $$d_{i}>1$$ then the state is called periodic with period $$d_{i}$$. If $$d_{i}=1$$ then the state is called aperiodic. If $$P_{ii}^{n}=0$$ for all $$n\\\\in\\\\mathbb{{N}}$$ then the period of the state $$i$$ is not defined.\",\n",
      "    \"State Periodicity is a Class Property\": \"Let $$(X_{{n}})_{n\\\\geq0}$$ be a Markov chain with the state space $$S$$ and the transition matrix $$P$$. Then the periodicity of states is a class property, that is all states in the same communicating class have the same period: $$i,j\\\\in S:\\\\quad i\\\\leftrightarrow j\\\\quad\\\\Longrightarrow\\\\quad d_i=d_j$$.\",\n",
      "    \"Irreducible Aperiodic Markov Chain\": \"Let $$(X_{{n}})_{n\\\\geq0}$$ be an irreducible Markov chain with the state space $$S$$ and the transition matrix $$P$$. Let $$d$$ denote the period of the states. If $$d>1$$ we say that the Markov chain is periodic with period $$d$$. If $$d=1$$ we say that the Markov chain is aperiodic.\",\n",
      "    \"Periodicity of Markov Chain\": \"The periodicity of the Markov chain $$(X_{{n}}){{n\\\\geq0}}$$ enabled us to compute $$\\\\mathbb{{P}}{{A}}(X_{{2024}}=D)$$ by calculating the $$7L$$ -th power of a $$2\\\\times2$$ rather than of a $$7\\\\times7$$ matrix.\",\n",
      "    \"Exercise 3.2.1\": \"Let $$(X_{{n}})_{{n\\\\geq0}}$$ be an irreducible Markov chain with the state space $$S={A,B,C,D}$$ and the transition matrix $$P$$ given by $$P=\\\\begin{{bmatrix}}0&0&1&0\\\\0&0&0&1\\\\0.5&0.5&0&0\\\\1&0&0&0\\\\end{{bmatrix}}.$$$$\",\n",
      "    \"Cyclic Classes\": \"All states communicate hence the chain is irreducible. There is only one communicating class. All states have the same period.\",\n",
      "    \"Period of Chain\": \"$$d_{{A}} =\\\\gcd{{2,4,6,8,10,\\\\ldots}}=2$$, $$d_{{B}} =\\\\gcd{{4,6,8,10,\\\\ldots}}=2$$, $$d_{{C}} =\\\\gcd{{2,4,6,8,10,\\\\ldots}}=2$$, $$d_{{D}} =\\\\gcd{{4,6,8,10,\\\\ldots}}=2$$. The period of the chain is 2.\",\n",
      "    \"Cyclic Classes (Continued)\": \"Cyclic classes: $${A,B}$$ and $${C,D}$$.\",\n",
      "    \"Transition Matrix in Block Form\": \"$$P=\\\\begin{{bmatrix}}0&I\\\\Q&0\\\\end{{bmatrix}}$$.\",\n",
      "    \"Powers of Transition Matrix\": \"$$P^{{2n}}=\\\\begin{{bmatrix}}Q^n&0\\\\0&Q^n\\\\end{{bmatrix}},\\\\quad P^{{2n+1}}=\\\\begin{{bmatrix}}0&Q^n\\\\Q^{{n+1}}&0\\\\end{{bmatrix}},\\\\quad n\\\\in\\\\mathbb{{N}}$$.\",\n",
      "    \"Definition 3.3.1\": \"Hitting time. Let $$(X_{{n}})_{{n\\\\geq0}}$$ be a Markov chain with the state space $$S$$. The first hitting time of a set $$A\\\\subseteq S$$ is the random variable $$H_{{A}}$$ defined by $$H_A=\\\\left{{\\\\begin{{array}}{{cl}}\\\\min{{n\\\\in\\\\mathbb{{N}}\\\\cup{{0}}:X_n\\\\in A}}&\\\\text{{if}}X_n\\\\in A\\\\text{{for some}}n\\\\in\\\\mathbb{{N}}\\\\cup{{0}},\\\\+\\\\infty&\\\\text{{if}}X_n\\\\notin A\\\\text{{for all}}n\\\\in\\\\mathbb{{N}}\\\\cup{{0}}.\\\\end{{array}}\\\\right.$$\",\n",
      "    \"Definition 3.3.2\": \"Hitting probabilities and expected hitting time. Let $$(X_{{n}})_{{n\\\\geq0}}$$ be a Markov chain with the state space $$S$$ and let $$A\\\\subseteq S$$. The probability that the Markov chain starting at state $$i\\\\in S$$ ever enters set $$A$$ is denoted by $$h_{{iA}}$$ and is given by $$h_{{iA}}=\\\\mathbb{{P}}(H_A<\\\\infty|X_0=i)=\\\\sum_{{n\\\\in\\\\mathbb{{N}}\\\\cup{{0}}}}\\\\mathbb{{P}}(H_A=n|X_0=i).$$\",\n",
      "    \"Proposition 3.3.1\": \"Let $$(X_{{n}}){{n\\\\geq0}}$$ be a Markov chain with the state space S and the transition matrix $$P$$. Let $$A\\\\subseteq S$$ be an arbitrary set. Then $${{h{{iA}},i\\\\in S}}$$ is the minimal non-negative solution of the system of linear equations.\",\n",
      "    \"Hitting Time Examples\": \"Example 3.3.1: Consider the Markov chain in Example 2.4.1. The transition matrix of the chain is given by $$P=\\\\begin{{bmatrix}}0.25&0.5&0.25\\\\0.5&0.25&0.25\\\\0.25&0.25&0.5\\\\end{{bmatrix}}.$$$$\",\n",
      "    \"Example 3.3.2\": \"Gambler's ruin problem (revisited): Recall the gambler's ruin problem presented in Example 2.3.2: a gambler has $$\\\\mathcal{L}k$$, $$k\\\\in\\\\mathbb{N}$$. They play a series of betting games.\",\n",
      "    \"Strong Markov Property\": \"The Markov property states that the chain starts afresh from the present time, but under certain conditions, it can start afresh from a random time, which is known as the strong Markov property.\",\n",
      "    \"Boundary Conditions\": \"1=v_0=A+B, 0=v_N=A\\\\left(\\\\frac{{q}}{{p}}\\\\right)^N+B.\",\n",
      "    \"Probability of Bankruptcy\": \"The probability that the gambler starting with n goes bankrupt before reaching LN is given by v_n=\\\\mathbb{P}(H_0<H_N|X_0=n)=A\\\\left(\\\\frac{{q}}{{p}}\\\\right)^{n}+B.\",\n",
      "    \"Probability of Reaching Before Bankruptcy\": \"The probability that the gambler starting with \\\\mathcal{L}^{\\\\prime}n reaches LN before going bankrupt is given by \\\\mathbb{P}(H_N<H_0|X_0=n)=1-v_n=\\\\frac{{1-\\\\left(\\\\frac{{q}}{{p}}\\\\right)^n}}{{1-\\\\left(\\\\frac{{q}}{{p}}\\\\right)^N}}.\",\n",
      "    \"Definition 3.4.1. Stopping Time\": \"A random variable T is a stopping time if for all n\\\\in\\\\mathbb{N}\\\\cup{0}, the event {T=n} depends only on random variables X_{0},X_{1},\\\\ldots,X_{n}.\",\n",
      "    \"Proposition 3.4.1. Strong Markov Property\": \"Conditional on {X_T=i} and {T<\\\\infty}, (X_{T+n})_{n\\\\geq0} is a Markov chain starting at i with the transition matrix P.\",\n",
      "    \"Definition 3.5.1. First Return Time\": \"The first return time of the Markov chain (X_{n})_{n\\\\geq0} to state i\\\\in S is the random variable T_i defined by T_i=\\\\min{n\\\\in\\\\mathbb{N}:X_n=i}.\",\n",
      "    \"Definition 3.5.2. Return Probability and Expected Return Time\": \"The probability that the Markov chain starting at state i\\\\in S returns to state \\\\dot{x} is denoted by f_i and is given by f_i=\\\\mathbb{P}(T_i<\\\\infty|X_0=i).\",\n",
      "    \"Definition 3.5.3. Number of Visits\": \"Let (X_{n})_{n\\\\geq0} be a Markov chain with the state space S. The number of times that the chain visits state i is defined by V_i=\\\\sum_{n=0}^\\\\infty I_{X_n=i}.\",\n",
      "    \"Example 3.5.1\": \"Let (X_{n})_{n\\\\geq0} be a Markov chain with the state space S={A,B,C,D} and the transition matrix P. We compute f_B=\\\\mathbb{P}(T_B<\\\\infty|X_0=B) and m_B=\\\\mathbb{E}(T_B|X_0=B).\",\n",
      "    \"Proposition 3.5.1\": \"For arbitrary i\\\\in S, f_i=1 if and only if \\\\mathbb{P}(V_i=\\\\infty|X_0=i)=1.\",\n",
      "    \"Recurrent and Transient States\": \"We distinguish between recurrent states (visited infinitely many times) and transient states (visited finitely many times).\",\n",
      "    \"Example 3.6.1\": \"In the given Markov chain, f_A=f_B=\\\\frac{1}{2} indicates a positive probability of never returning to states A and B, while states C and D are visited infinitely many times.\",\n",
      "    \"Return Probability Approximation\": \"For a random walk, $$P_{00}^{2n} \\\\sim \\\\frac{1}{\\\\sqrt{\\\\pi n}}$$ as $$n \\\\to \\\\infty$$ when $$p = q = \\\\frac{1}{2}$$.\",\n",
      "    \"Markov Chain Properties\": \"If the chain starts at state 0 and returns to 0 in $$2n$$ steps, the chain made n steps to the right with probability $$p$$ and n steps to the left with probability $$q$$. The number of ways to choose n steps out of $$2n$$ is given by $$\\\\binom{2n}{n}$$.\",\n",
      "    \"Stirling's Formula\": \"We use Stirling's formula to approximate $$n!$$ for large $$n$$: $$n! \\\\sim \\\\sqrt{2\\\\pi n} \\\\left(\\\\frac{n}{e}\\\\right)^n$$ as $$n \\\\to \\\\infty$$.\",\n",
      "    \"Recurrence and Transience\": \"Let $$(X_{n})_{n\\\\geq0}$$ be a Markov chain with the state space $$S$$. Let $$C\\\\subseteq S$$ be a communicating class. Then either all states in $$C$$ are recurrent or are all transient.\",\n",
      "    \"Example of Recurrence\": \"For the simple symmetric random walk on $$Z$$, if $$p = q = \\\\frac{1}{2}$$, then state 0 is recurrent.\",\n",
      "    \"Example of Transience\": \"If $$p \\\\neq q$$, then the random walk on $$Z$$ is transient. The probability of returning to state 0 decreases, and we find $$P_{00}^{2n} < \\\\frac{r^n}{\\\\sqrt{\\\\pi}}$$ for some $$r < 1$$.\",\n",
      "    \"First Step Decomposition\": \"By the first step decomposition, $$\\\\mathbb{P}(T_0 < \\\\infty | X_0 = 0) = p \\\\cdot \\\\mathbb{P}(H_0 < \\\\infty | X_1 = 1) + (1-p) \\\\cdot \\\\mathbb{P}(H_0 < \\\\infty | X_1 = -1)$$.\",\n",
      "    \"Proposition 3.6.1\": \"Let $$(X_{n})_{n\\\\geq0}$$ be a Markov chain with the state space $$S$$. If $$i\\\\in C$$ is recurrent, then for any $$j\\\\in C$$, $$j$$ is also recurrent.\",\n",
      "    \"Definition 3.6.2\": \"Let $$(X_{n})_{n\\\\geq0}$$ be an irreducible Markov chain. If all states of $$(X_{n})_{n\\\\geq0}$$ are transient, then it is called a transient Markov chain. If all states are recurrent, then it is called a recurrent Markov chain.\",\n",
      "    \"Example 3.6.2\": \"A simple random walk on $$\\\\mathbb{Z}$$ is recurrent if $$p=q=\\\\frac{1}{2}$$ and transient if $$p\\\\neq q$$.\",\n",
      "    \"Example 3.6.4\": \"A simple symmetric two-dimensional random walk on $$\\\\mathbb{Z}^2$$ is recurrent.\",\n",
      "    \"Example 3.6.5\": \"For the Markov chain with state space $$S={A,B,C,D}$$, classes $${A,B}$$ is transient and class $${C,D}$$ is recurrent.\",\n",
      "    \"Expected Value Equations\": {\n",
      "        \"For A\": \"$$$$\\\\mathbb{E}(V_A|X_0=A)+\\\\mathbb{E}(V_B|X_0=A)=\\\\frac{8}{5}$$$$\",\n",
      "        \"For B\": \"$$$$\\\\mathbb{E}(V_A|X_0=B)+\\\\mathbb{E}(V_B|X_0=B)=\\\\frac{9}{5}$$$$\"\n",
      "    },\n",
      "    \"Transition Probabilities\": {\n",
      "        \"h_AC\": \"$$$$h_{AC}=\\\\frac{2}{3}+\\\\frac{1}{3}h_{BC}$$$$\",\n",
      "        \"h_BC\": \"$$$$h_{BC}=\\\\frac{1}{8} + \\\\frac{1}{2}h_{AC}$$$$\",\n",
      "        \"h_AD\": \"$$$$h_{AD}=\\\\frac{1}{3}h_{BD}$$$$\",\n",
      "        \"h_BD\": \"$$$$h_{BD}=\\\\frac{3}{8} + \\\\frac{1}{2}h_{AD}$$$$\"\n",
      "    },\n",
      "    \"Probabilities of Hitting States\": {\n",
      "        \"From A to C\": \"$$$$\\\\frac{17}{20}$$$$\",\n",
      "        \"From A to D\": \"$$$$\\\\frac{3}{20}$$$$\",\n",
      "        \"From B to C\": \"$$$$\\\\frac{11}{20}$$$$\",\n",
      "        \"From B to D\": \"$$$$\\\\frac{9}{20}$$$$\"\n",
      "    },\n",
      "    \"Communicating Classes\": {\n",
      "        \"Classes\": \"$${{A,B}}, {{C,D}}, {{E,F}}$$\",\n",
      "        \"Recurrent Classes\": \"$${{C,D}}, {{E,F}}$$\",\n",
      "        \"Transient Class\": \"$${{A,B}}$$\"\n",
      "    },\n",
      "    \"Expected Values\": {\n",
      "        \"From A\": {\n",
      "            \"V_A\": \"$$$$\\\\mathbb{E}(V_A|X_0=A)=\\\\frac{50}{41}$$$$\",\n",
      "            \"V_B\": \"$$$$\\\\mathbb{E}(V_B|X_0=A)=\\\\frac{30}{41}$$$$\"\n",
      "        },\n",
      "        \"From B\": {\n",
      "            \"V_A\": \"$$$$\\\\mathbb{E}(V_A|X_0=B)=\\\\frac{15}{41}$$$$\",\n",
      "            \"V_B\": \"$$$$\\\\mathbb{E}(V_B|X_0=B)=\\\\frac{50}{41}$$$$\"\n",
      "        }\n",
      "    },\n",
      "    \"Expected Time Until Leaving Set AB\": {\n",
      "        \"From A\": \"$$$$\\\\frac{80}{41}$$$$\",\n",
      "        \"From B\": \"$$$$\\\\frac{65}{41}$$$$\"\n",
      "    },\n",
      "    \"Probability of Hitting E,F Before C,D\": {\n",
      "        \"From A\": \"$$$$\\\\frac{12}{41}$$$$\"\n",
      "    },\n",
      "    \"Probability of Hitting C\": {\n",
      "        \"From A\": \"$$$$\\\\frac{29}{41}$$$$\"\n",
      "    },\n",
      "    \"Definitions\": {\n",
      "        \"Communicating States\": \"Let $$(X_{n})_{n\\\\geq0}$$ be a Markov chain with the state space S and the transition matrix $$P$$. We say that state $$j$$ is accessible from state $$i$$ and write $$i\\\\to j$$ if there exists $$n\\\\in\\\\mathbb{N}\\\\cup{0}$$ such that $$P_{ij}^{n}>0$$.\",\n",
      "        \"Communicating Class\": \"A communicating class $$C\\\\subseteq S$$ is a maximal collection of communicating states such that for any $$i,j\\\\in C$$, $$i\\\\leftrightarrow j$$.\",\n",
      "        \"Irreducible Markov Chain\": \"A Markov chain is called irreducible if the entire state space is one communicating class.\",\n",
      "        \"Closed Class\": \"A communicating class is called closed if no states outside the class are accessible from the states within the class.\",\n",
      "        \"Absorbing State\": \"A state is called absorbing if it forms a closed class by itself.\",\n",
      "        \"Hitting Time\": \"The first hitting time of a set $$A\\\\subseteq S$$ is the random variable $$H_A$$ defined by $$H_A=\\\\min{n\\\\in\\\\mathbb{N}\\\\cup{0}:X_n\\\\in A}$$.\"\n",
      "    }\n",
      "}\n"
     ]
    },
    {
     "ename": "APIResponseError",
     "evalue": "body failed validation: body.children[152].toggle.children[0].paragraph.rich_text[0].text.content should be a string, instead was `{\"For A\":\"$$$$\\\\mathbb{E}(V_A|X_0=A)+\\\\mathbb{E}(V_B|X_...`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Zain\\Documents\\Github\\University\\.venv\\Lib\\site-packages\\notion_client\\client.py:118\u001b[0m, in \u001b[0;36mBaseClient._parse_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 118\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[1;32mc:\\Users\\Zain\\Documents\\Github\\University\\.venv\\Lib\\site-packages\\httpx\\_models.py:763\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m message \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m, error_type\u001b[38;5;241m=\u001b[39merror_type)\n\u001b[1;32m--> 763\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request\u001b[38;5;241m=\u001b[39mrequest, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPStatusError\u001b[0m: Client error '400 Bad Request' for url 'https://api.notion.com/v1/blocks/116a89672d99807781b1eb665cb64ad6/children'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/400",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAPIResponseError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexperiment\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m formatted_datetime\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFolder found in experiments/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpageID\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 8\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(filename, pageID)\u001b[0m\n\u001b[0;32m      6\u001b[0m Extractor(filename)\u001b[38;5;241m.\u001b[39mrun()\n\u001b[0;32m      7\u001b[0m Squasher(filename)\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m----> 8\u001b[0m \u001b[43mNotion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpageID\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Zain\\Documents\\Github\\University\\src\\notion.py:20\u001b[0m, in \u001b[0;36mNotion.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     18\u001b[0m     toggle: \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_template(key, value)\n\u001b[0;32m     19\u001b[0m     form\u001b[38;5;241m.\u001b[39mappend(toggle)\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mform\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Zain\\Documents\\Github\\University\\src\\notion.py:51\u001b[0m, in \u001b[0;36mNotion._setup\u001b[1;34m(self, blocks)\u001b[0m\n\u001b[0;32m     49\u001b[0m     NOTION_TOKEN \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOTION_TOKEN\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     50\u001b[0m     notion \u001b[38;5;241m=\u001b[39m Client(auth\u001b[38;5;241m=\u001b[39mNOTION_TOKEN)\n\u001b[1;32m---> 51\u001b[0m     \u001b[43mnotion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchildren\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblock_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchildren\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblocks\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Zain\\Documents\\Github\\University\\.venv\\Lib\\site-packages\\notion_client\\api_endpoints.py:23\u001b[0m, in \u001b[0;36mBlocksChildrenEndpoint.append\u001b[1;34m(self, block_id, **kwargs)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mappend\u001b[39m(\u001b[38;5;28mself\u001b[39m, block_id: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SyncAsync[Any]:\n\u001b[0;32m     19\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create and append new children blocks to the block using the ID specified.\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03m    *[🔗 Endpoint documentation](https://developers.notion.com/reference/patch-block-children)*\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mblocks/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mblock_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/children\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPATCH\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpick\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchildren\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mafter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Zain\\Documents\\Github\\University\\.venv\\Lib\\site-packages\\notion_client\\client.py:194\u001b[0m, in \u001b[0;36mClient.request\u001b[1;34m(self, path, method, query, body, auth)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException:\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestTimeoutError()\n\u001b[1;32m--> 194\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Zain\\Documents\\Github\\University\\.venv\\Lib\\site-packages\\notion_client\\client.py:126\u001b[0m, in \u001b[0;36mBaseClient._parse_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    124\u001b[0m         code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m code \u001b[38;5;129;01mand\u001b[39;00m is_api_error_code(code):\n\u001b[1;32m--> 126\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m APIResponseError(response, body[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m], code)\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPResponseError(error\u001b[38;5;241m.\u001b[39mresponse)\n\u001b[0;32m    129\u001b[0m body \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "\u001b[1;31mAPIResponseError\u001b[0m: body failed validation: body.children[152].toggle.children[0].paragraph.rich_text[0].text.content should be a string, instead was `{\"For A\":\"$$$$\\\\mathbb{E}(V_A|X_0=A)+\\\\mathbb{E}(V_B|X_...`."
     ]
    }
   ],
   "source": [
    "pageURL = \"https://zain273work.notion.site/Stochastic-Processes-116a89672d99807781b1eb665cb64ad6?pvs=4\"\n",
    "pageID = extract_code_from_url(pageURL)\n",
    "print(pageID)\n",
    "from datetime import datetime\n",
    "current_datetime = datetime.now()\n",
    "\n",
    "# Format it as a string\n",
    "formatted_datetime: str = current_datetime.strftime(\"%Y%m%d%H%M%S\")\n",
    "name: str = \"experiment\" + formatted_datetime\n",
    "print(f\"Folder found in experiments/{name}\")\n",
    "pipeline(name, pageID)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
